# -*- coding: utf-8 -*-


"""
========================
Random Walks on Networks
========================

:Author:
    Moritz Emanuel Beber
:Date:
    2012-10-23
:Copyright:
    Copyright |c| 2013, Jacobs University Bremen gGmbH, all rights reserved.
:File:
    walkers.py

.. |c| unicode:: U+A9
"""


__all__ = ["UniformInterval", "ConstantValue", "DegreeDependentValue",
        "compute_mu", "prepare_uniform_walk", "uniform_random_walker",
        "iterative_parallel_march",
        "parallel_march", "deletory_parallel_march",
        "buffered_parallel_march", "internal_dynamics_external_fluctuations"]

#        "limited_uniform_random_walker",

import sys
import itertools
import bisect

import numpy
import networkx as nx

from IPython.parallel import interactive, require, LoadBalancedView


class UniformInterval(object):
    """
    Instances of UniformInterval can be called without argument to yield a
    natural number from a uniform random distribution on a pre-specified
    interval (cut off at zero).
    """

    def __init__(self, mid_point, variation=0, **kw_args):
        """
        At instantiation the mean of the interval and and the uniform variation
        around that mean are determined.

        Parameters
        ----------
        mid_point: int
            The mean of the uniform random variable.
        variation: int (optional)
            Determines the interval, it is from mid_point - variation or zero to
            mid_point + variation.
        """
        super(UniformInterval, self).__init__(**kw_args)
        self.rand_int = numpy.random.random_integers
        self.mid_point = int(mid_point)
        assert self.mid_point >= 0
        self.variation = int(variation)
        assert self.variation >= 0
        self.mini = self.mid_point - self.variation
        # This definition leads to a potential accumulation of zero values and
        # thus a non-uniform probability density function.
        # A uniform PDF over the interval is guaranteed by:
#        self.mini = max(self.mid_point - self.variation, 0)
        # but with a changed mean and variance
        self.maxi = self.mid_point + self.variation

    def __call__(self):
        if self.variation > 0:
            self.__call__ = self.variable
        else:
            self.__call__ = self.constant
        return self.__call__()

    def constant(self):
        return self.mid_point

    def variable(self):
        return max(self.rand_int(self.mini, self.maxi), 0)


class ConstantValue(object):
    """
    An estimator of the value generated by a random walker visiting a node.
    """

    def __init__(self, constant=1.0, **kw_args):
        """
        Instantiation determines the constant value.
        """
        super(ConstantValue, self).__init__(**kw_args)
        self.value = float(constant)

    def __call__(self, *args):
        """
        Function definition ignores any positional arguments for compatibility
        reasons.
        """
        return self.value


class DegreeDependentValue(object):
    """
    A degree-dependent estimator of the value generated by a random walker
    visiting a node.

    References
    ----------
    .. [1] Eisler, Z., and J. Kertész.
           “Random Walks on Complex Networks with Inhomogeneous Impact.”
           *Phys. Rev. E* 71, no. 5 (May 2005): 057104. doi:10.1103/PhysRevE.71.057104.
    """

    def __init__(self, graph, indices, weight=None, mu=1.0, **kw_args):
        """
        Estimates the value of nodes based on their (weighted) degree and an
        exponent.

        Parameters
        ----------
        graph: nx.(Di)Graph
            The graph from which to obtain the degree of the nodes.
        indices: dict
            A map from nodes to their integer indeces [0; N - 1].
        weight: str (optional)
            The keyword for the edge-weight attribute.
        mu: float (optional)
            The exponent of the value.
        """
        super(DegreeDependentValue, self).__init__(**kw_args)
        self.values = numpy.zeros(len(graph), dtype=float)
        mu = float(mu)
        for (node, deg) in graph.degree_iter(weight=weight):
            self.values[indices[node]] = numpy.power(float(deg), mu)

    def __call__(self, index, *args):
        """
        Calls with the index of a node return its pre-computed value.
        """
        return self.values[index]

def compute_mu(alpha, nu=1.0):
    """
    In [\ 1_] the power-law exponent $\alpha$ is determined by the values of $\mu$
    and $\nu$. This function computes $\mu$ such that a given exponent $\alpha$
    can be recovered.

    $\alpha$ is determined in the following way:
    \begin{equation}
        \alpha = \dfrac{1}{2} \left( 1 + \dfrac{\dfrac{\mu}{\nu}}{\dfrac{\mu}{\nu} + 1} \right).
    \end{equation}

    According to Mathematica, the solution for $\mu$ is:
    \begin{equation}
        \mu = \dfrac{\nu - 2 \alpha \nu}{2 \left( \alpha - 1 \right)}.
    \end{equation}

    References
    ----------
    .. [1] Eisler, Z., and J. Kertész.
           “Random Walks on Complex Networks with Inhomogeneous Impact.”
           *Phys. Rev. E* 71, no. 5 (May 2005): 057104. doi:10.1103/PhysRevE.71.057104.
    """
    alpha = float(alpha)
    nu = float(nu)
    return (nu - (2.0 * alpha * nu)) / (2.0 * (alpha - 1.0))

def prepare_uniform_walk(graph, node2id=None, weight=None):
    """
    Prepare data structures for a uniform random walk.

    Works for undirected as well as directed graphs. For multi-graphs simply
    provide a suitable edge weight.

    Parameters
    ----------
    graph: nx.(Di)Graph
        The underlying network.
    node2id: dict
        A mapping from nodes in graph to indices running from 0 to (N - 1).
    weight: hashable
        The keyword for edge data that should be used to weigh the propagation
        probability.
    """
    nodes = sorted(graph.nodes())
    if len(nodes) < 2:
        raise nx.NetworkXError("network is too small")
    if node2id is None:
        node2id = dict(itertools.izip(nodes, itertools.count()))
    probabilities = range(len(nodes))
    neighbours = range(len(nodes))
    for node in nodes:
        i = node2id[node]
        adj = graph[node]
        out_deg = len(adj)
        if out_deg == 0:
            probabilities[i] = list()
            neighbours[i] = list()
            continue
        prob = 0.0
        probabilities[i] = numpy.zeros(out_deg, dtype=float)
        neighbours[i] = numpy.zeros(out_deg, dtype=int)
        for (j, (nhbr, data)) in enumerate(adj.iteritems()):
            neighbours[i][j] = node2id[nhbr]
            prob += data.get(weight, 1.0)
            probabilities[i][j] = prob
        # prob is now the sum of all edge weights, normalise to unity
        probabilities[i] /= prob
    return (probabilities, neighbours, node2id)

@require(numpy, bisect)
@interactive
def uniform_random_walker(node):
    """
    Perform a single random walk on a network with a uniform probability of a
    next step.

    Parameters
    ----------
    """
    # accessing globals `probabilities`, `neighbours`, and `steps that were pushed before
    local_probs = probabilities
    local_nbrs = neighbours
    smpl = numpy.random.random_sample
    choose = bisect.bisect_left
    path = [node]
    for s in xrange(steps):
        nbrs = local_nbrs[node]
        if len(nbrs) == 0:
            break
        draw = smpl()
        # the nbrs list and probs list correspond to each other
        # we use a binary search to find the index to the left of the
        # probability and take that node
        node = nbrs[choose(local_probs[node], draw)]
        path.append(node)
    return path

#@require(numpy, bisect)
#@interactive
#def limited_uniform_random_walker(node, max_steps=None):
#    """
#    Perform a single random walk on a network with a uniform probability of a
#    next step.
#
#    Parameters
#    ----------
#    """
#    # accessing globals `probabilities`, `neighbours`, and `steps` that were pushed before
#    local_probs = probabilities
#    local_nbrs = neighbours
#    if max_steps is None:
#        max_steps = steps
#    smpl = numpy.random.random_sample
#    choose = bisect.bisect_left
#    path = [node]
#    for s in xrange(max_steps):
#        nbrs = local_nbrs[node]
#        if len(nbrs) == 0:
#            break
#        draw = smpl()
#        # the nbrs list and probs list correspond to each other
#        # we use a binary search to find the index to the left of the
#        # probability and take that node
#        node = nbrs[choose(local_probs[node], draw)]
#        path.append(node)
#    return path

def clear_client(rc):
    """
    Particularly with older versions of IPython memory becomes a huge issue.

    Frequent use of this function should alleviate the issue.
    """
    assert not rc.outstanding, "Can't clear a client with outstanding tasks!"
    rc.results.clear()
    rc.metadata.clear()
    rc.history = list()
    rc.session.digest_history.clear()

def clear_view(view):
    """
    Particularly with older versions of IPython memory becomes a huge issue.

    Frequent use of this function should alleviate the issue.
    """
    view.results.clear()
    view.history = list()

def iterative_parallel_march(d_view, neighbours, probabilities, sources, num_walkers, time_points,
        steps, assessor=ConstantValue(), transient=0, lb_view=None, seed=None):
    """
    Start a number of random walks on the given network for a number of time points
    and compute running mean and standard deviation of the visits at each node.

    Parameters
    ----------
    d_view: DirectView
        An IPython.parallel.DirectView instance.
    neighbours: list of lists
        Adjacency list structure as returned by prepare_uniform_walk.
    probabilities: list of lists
        Transition probabilities list structure as returned by
        prepare_uniform_walk.
    sources: list
        List of valid starting node indices.
    num_walkers: callable
        A callable that returns an integer z >= 0.
    time_points: int
        Number of experiments to measure activity for.
    steps: int
        The maximum number of steps for each individual random walker.
    assessor: callable (optional)
        Called with the node index as argument, it should return the activity
        value of a visit.
    transient: int (optional)
        Cut-off the first transient steps of each random walk.
    lb_view: LoadBalancedView (optional)
        An IPython.parallel.LoadBalancedView instance which may have performance
        advantages over a DirectView.
    seed: (optional)
        A valid seed for numpy.random that makes runs deterministic in
        combination with using only a DirectView.


    Warning
    -------
    The use of a seed for reproducible results can only work with a
    ``DirectView``. Use of a ``LoadBalancedView`` will assign jobs to remote
    kernels in unknown order.
    """
    time_points = int(time_points)
    steps = int(steps)
    transient = int(transient)
    length = len(sources)
    rand_int = numpy.random.randint
    # compute a running mean and sd as per:
    # http://en.wikipedia.org/wiki/Standard_deviation#Rapid_calculation_methods
    visits = numpy.zeros(len(neighbours))
    mean_fluxes = numpy.zeros(len(neighbours))
    subtraction = numpy.zeros(len(neighbours))
    std_fluxes = numpy.zeros(len(neighbours))
    # make available on remote kernels
    d_view.push(dict(neighbours=neighbours, probabilities=probabilities,
        steps=steps), block=True)
    # assign different but deterministic seeds to all remote engines
    numpy.random.seed(seed)
    remote_seeds = set()
    while len(remote_seeds) < len(d_view):
        remote_seeds.add(rand_int(sys.maxint))
    d_view.scatter("seed", remote_seeds, block=True)
    d_view.execute("import numpy", block=True)
    d_view.execute("numpy.random.seed(seed[0])", block=True)
    view = isinstance(lb_view, LoadBalancedView)
    if view:
        num_krnl = len(lb_view)
    sys.stdout.write("\r{0:7.2%} complete".format(0.0))
    sys.stdout.flush()
    time_norm = float(time_points)
    for time in xrange(1, time_points + 1):
        visits.fill(0)
        curr_num = num_walkers()
        if curr_num == 0:
            subtraction = -mean_fluxes
            mean_fluxes += subtraction / time
            std_fluxes += subtraction * (-mean_fluxes)
            sys.stdout.write("\r{0:7.2%} complete".format(time / time_norm))
            sys.stdout.flush()
            continue
        if view:
            size = max((curr_num - 1) // (num_krnl * 2), 1)
            results = lb_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False, ordered=False, chunksize=size)
        else:
            results = d_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False)
        for path in results:
            for node in path[transient:]:
                visits[node] += assessor(node)
        # clear cache
        clear_client(d_view.client)
        if view:
            clear_view(lb_view)
        clear_view(d_view)
        # compute running average and variation
        subtraction = visits - mean_fluxes
        mean_fluxes += subtraction / time
        std_fluxes += subtraction * (visits - mean_fluxes)
        sys.stdout.write("\r{0:7.2%} complete".format(time / time_norm))
        sys.stdout.flush()
    std_fluxes /= float(time - 1)
    numpy.sqrt(std_fluxes, std_fluxes)
    sys.stdout.write("\r{0:7.2%} complete".format(1.0))
    sys.stdout.write("\n")
    sys.stdout.flush()
    return (mean_fluxes, std_fluxes)

def parallel_march(d_view, neighbours, probabilities, sources, num_walkers, time_points,
        steps, assessor=ConstantValue(), transient=0, lb_view=None, seed=None):
    """
    Start a number of random walks on the given network for a number of time
    points. Records the activity at visited nodes.

    Parameters
    ----------
    d_view: DirectView
        An IPython.parallel.DirectView instance.
    neighbours: list of lists
        Adjacency list structure as returned by prepare_uniform_walk.
    probabilities: list of lists
        Transition probabilities list structure as returned by
        prepare_uniform_walk.
    sources: list
        List of valid starting node indices.
    num_walkers: callable
        A callable that returns an integer z >= 0.
    time_points: int
        Number of experiments to measure activity for.
    steps: int
        The maximum number of steps for each individual random walker.
    assessor: callable (optional)
        Called with the node index as argument, it should return the activity
        value of a visit.
    transient: int (optional)
        Cut-off the first transient steps of each random walk.
    lb_view: LoadBalancedView (optional)
        An IPython.parallel.LoadBalancedView instance which may have performance
        advantages over a DirectView.
    seed: (optional)
        A valid seed for numpy.random that makes runs deterministic in
        combination with using only a DirectView.

    Returns
    -------
    An array of dimensions number of nodes N x number of time points T.

    Warning
    -------
    The use of a seed for reproducible results can only work with a
    ``DirectView``. Use of a ``LoadBalancedView`` will assign jobs to remote
    kernels in unknown order.
    """
    time_points = int(time_points)
    steps = int(steps)
    transient = int(transient)
    length = len(sources)
    rand_int = numpy.random.randint
    visits = numpy.zeros(shape=(len(neighbours), time_points), dtype=float)
    sys.stdout.flush()
    # make available on remote kernels
    d_view.push(dict(neighbours=neighbours, probabilities=probabilities,
        steps=steps), block=True)
    # assign different but deterministic seeds to all remote engines
    numpy.random.seed(seed)
    remote_seeds = set()
    while len(remote_seeds) < len(d_view):
        remote_seeds.add(rand_int(sys.maxint))
    d_view.scatter("seed", remote_seeds, block=True)
    d_view.execute("import numpy", block=True)
    d_view.execute("numpy.random.seed(seed[0])", block=True)
    view = isinstance(lb_view, LoadBalancedView)
    if view:
        num_krnl = len(lb_view)
    sys.stdout.write("\r{0:7.2%} complete".format(0.0))
    sys.stdout.flush()
    time_norm = float(time_points)
    for time in xrange(time_points):
        curr_visits = visits[:, time]
        curr_num = num_walkers()
        if curr_num == 0:
            sys.stdout.write("\r{0:7.2%} complete".format(time / time_norm))
            sys.stdout.flush()
            continue
        if view:
            size = max((curr_num - 1) // (num_krnl * 2), 1)
            results = lb_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False, ordered=False, chunksize=size)
        else:
            results = d_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False)
        for path in results:
            for node in path[transient:]:
                curr_visits[node] += assessor(node)
        # clear cache
        clear_client(d_view.client)
        if view:
            clear_view(lb_view)
        clear_view(d_view)
        sys.stdout.write("\r{0:7.2%} complete".format(time / time_norm))
        sys.stdout.flush()
    sys.stdout.write("\r{0:7.2%} complete".format(1.0))
    sys.stdout.write("\n")
    sys.stdout.flush()
    return visits

def deletory_parallel_march(d_view, neighbours, probabilities, sources,
        num_walkers, time_points, steps, capacity, assessor=ConstantValue(),
        transient=0, lb_view=None, seed=None):
    """
    Start a number of random walks on the given network for a number of time
    points. Records the activity at visited nodes.

    Parameters
    ----------
    d_view: DirectView
        An IPython.parallel.DirectView instance.
    neighbours: list of lists
        Adjacency list structure as returned by prepare_uniform_walk.
    probabilities: list of lists
        Transition probabilities list structure as returned by
        prepare_uniform_walk.
    sources: list
        List of valid starting node indices.
    num_walkers: callable
        A callable that returns an integer z >= 0.
    time_points: int
        Number of experiments to measure activity for.
    steps: int
        The maximum number of steps for each individual random walker.
    capacity: list or dict
        Contains maximum capacity of nodes at their respective index.
    assessor: callable (optional)
        Called with the node index as argument, it should return the activity
        value of a visit.
    transient: int (optional)
        Cut-off the first transient steps of each random walk.
    lb_view: LoadBalancedView (optional)
        An IPython.parallel.LoadBalancedView instance which may have performance
        advantages over a DirectView.
    seed: (optional)
        A valid seed for numpy.random that makes runs deterministic in
        combination with using only a DirectView.

    Returns
    -------
    An array of dimensions number of nodes N x number of time points T that
    records the activity at each node per time point. An array of size T that
    measures the number of removed walkers.

    Warning
    -------
    The use of a seed for reproducible results can only work with a
    ``DirectView``. Use of a ``LoadBalancedView`` will assign jobs to remote
    kernels in unknown order.
    """
    time_points = int(time_points)
    steps = int(steps)
    transient = int(transient)
    length = len(sources)
    rand_int = numpy.random.randint
    visits = numpy.zeros(shape=(len(neighbours), time_points), dtype=float)
    removed = numpy.zeros(shape=(len(neighbours), time_points), dtype=int)
    sys.stdout.flush()
    # make available on remote kernels
    d_view.push(dict(neighbours=neighbours, probabilities=probabilities,
        steps=steps), block=True)
    # assign different but deterministic seeds to all remote engines
    numpy.random.seed(seed)
    remote_seeds = set()
    while len(remote_seeds) < len(d_view):
        remote_seeds.add(rand_int(sys.maxint))
    d_view.scatter("seed", remote_seeds, block=True)
    d_view.execute("import numpy", block=True)
    d_view.execute("numpy.random.seed(seed[0])", block=True)
    view = isinstance(lb_view, LoadBalancedView)
    if view:
        num_krnl = len(lb_view)
    sys.stdout.write("\r{0:7.2%} complete".format(0.0))
    sys.stdout.flush()
    time_norm = float(time_points)
    for time in xrange(time_points):
        curr_visits = visits[:, time]
        curr_num = num_walkers()
        if curr_num == 0:
            sys.stdout.write("\r{0:7.2%} complete".format(time / time_norm))
            sys.stdout.write("\r{0:7.2%} complete, removed: {1:12d}".format(time / time_norm,
                    removed[:, time].sum()))
            sys.stdout.flush()
            continue
        if view:
            size = max((curr_num - 1) // (num_krnl * 2), 1)
            results = lb_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False, ordered=False, chunksize=size)
        else:
            results = d_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False)
        for path in results:
            # if transient > 0, the nodes visited in the transient are ignored
            for node in path[transient:]:
                if curr_visits[node] >= capacity[node]:
                    removed[node, time] += 1
                    break
                curr_visits[node] += assessor(node)
        # clear cache
        clear_client(d_view.client)
        if view:
            clear_view(lb_view)
        clear_view(d_view)
        sys.stdout.write("\r{0:7.2%} complete".format(time / time_norm))
        sys.stdout.write("\r{0:7.2%} complete, removed: {1:12d}".format(time / time_norm,
                removed[:, time].sum()))
        sys.stdout.flush()
    sys.stdout.write("\r{0:7.2%} complete".format(1.0))
    sys.stdout.write("\n")
    sys.stdout.flush()
    return (visits, removed)

def buffered_parallel_march(d_view, neighbours, probabilities, sources,
        num_walkers, time_points, steps, capacity, assessor=ConstantValue(),
        transient=0, lb_view=None, seed=None):
    """
    Start a number of random walks on the given network for a number of time
    points. Records the activity at visited nodes.

    Parameters
    ----------
    d_view: DirectView
        An IPython.parallel.DirectView instance.
    neighbours: list of lists
        Adjacency list structure as returned by prepare_uniform_walk.
    probabilities: list of lists
        Transition probabilities list structure as returned by
        prepare_uniform_walk.
    sources: list
        List of valid starting node indices.
    num_walkers: callable
        A callable that returns an integer z >= 0.
    time_points: int
        Number of experiments to measure activity for.
    steps: int
        The maximum number of steps for each individual random walker.
    capacity: dict
        Map between node indices and their maximum capacity.
    assessor: callable (optional)
        Called with the node index as argument, it should return the activity
        value of a visit.
    transient: int (optional)
        Cut-off the first transient steps of each random walk.
    lb_view: LoadBalancedView (optional)
        An IPython.parallel.LoadBalancedView instance which may have performance
        advantages over a DirectView.
    seed: (optional)
        A valid seed for numpy.random that makes runs deterministic in
        combination with using only a DirectView.

    Returns
    -------
    An array of dimensions number of nodes N x number of time points T that
    records the activity at each node per time point. An array of size T that
    measures the backlog at each time point.

    Warning
    -------
    The use of a seed for reproducible results can only work with a
    ``DirectView``. Use of a ``LoadBalancedView`` will assign jobs to remote
    kernels in unknown order.
    """
    time_points = int(time_points)
    steps = int(steps)
    transient = int(transient)
    length = len(sources)
    rand_int = numpy.random.randint
    visits = numpy.zeros(shape=(len(neighbours), time_points), dtype=float)
    backlog = numpy.zeros(shape=(len(neighbours), time_points), dtype=int)
    sys.stdout.flush()
    # make available on remote kernels
    d_view.push(dict(neighbours=neighbours, probabilities=probabilities,
        steps=steps), block=True)
    # assign different but deterministic seeds to all remote engines
    numpy.random.seed(seed)
    remote_seeds = set()
    while len(remote_seeds) < len(d_view):
        remote_seeds.add(rand_int(sys.maxint))
    d_view.scatter("seed", remote_seeds, block=True)
    d_view.execute("import numpy", block=True)
    d_view.execute("numpy.random.seed(seed[0])", block=True)
    view = isinstance(lb_view, LoadBalancedView)
    if view:
        num_krnl = len(lb_view)
    sys.stdout.write("\r{0:7.2%} complete".format(0.0))
    sys.stdout.flush()
    old_buffer = list()
    new_buffer = list()
    time_norm = float(time_points)
    for time in xrange(time_points):
        curr_visits = visits[:, time]
        curr_num = num_walkers()
        if curr_num == 0:
            old_buffer = new_buffer
            new_buffer = list()
            for path in old_buffer:
                # no need to cut transient since the buffered paths have been cut
                for (i, node) in enumerate(path):
                    if curr_visits[node] >= capacity[node]:
                        backlog[node, time] += 1
                        new_buffer.append(path[i:])
                        break
                    curr_visits[node] += assessor(node)
            sys.stdout.write("\r{0:7.2%} complete, backlog: {1:12d}".format(time / time_norm,
                len(new_buffer)))
            sys.stdout.flush()
            continue
        if view:
            size = max((curr_num - 1) // (num_krnl * 2), 1)
            results = lb_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False, ordered=False, chunksize=size)
        else:
            results = d_view.map(uniform_random_walker,
                    [sources[rand_int(length)] for i in xrange(curr_num)],
                    block=False)
        old_buffer = new_buffer
        new_buffer = list()
        for path in old_buffer:
            # no need to cut transient since the buffered paths have been cut
            for (i, node) in enumerate(path):
                if curr_visits[node] >= capacity[node]:
                    backlog[node, time] += 1
                    break
                curr_visits[node] += assessor(node)
            if i < len(path):
                new_buffer.append(path[i:])
        for path in results:
            path = path[transient:]
            # if transient > 0, the nodes visited in the transient are ignored
            for (i, node) in enumerate(path):
                if curr_visits[node] >= capacity[node]:
                    backlog[node, time] += 1
                    new_buffer.append(path[i:])
                    break
                curr_visits[node] += assessor(node)
        # clear cache
        clear_client(d_view.client)
        if view:
            clear_view(lb_view)
        clear_view(d_view)
        sys.stdout.write("\r{0:7.2%} complete, backlog: {1:12d}".format(time / time_norm,
            len(new_buffer)))
        sys.stdout.flush()
    sys.stdout.write("\r{0:7.2%} complete".format(1.0))
    sys.stdout.write("\n")
    sys.stdout.flush()
    return (visits, backlog)

def internal_dynamics_external_fluctuations(activity):
    """
    Assess the internal dynamics and external fluctuations of a complex system.

    Parameters
    ----------
    activity: numpy.array
        Two dimensional array, where the first dimension corresponds to system
        elements and the second to observations of their activity.

    Returns
    -------
    Two arrays for the standard deviation of the internal and external
    fluctuations of components, respectively.

    References
    ----------
    """
    internal = numpy.zeros(activity.shape, dtype=float)
    external = numpy.zeros(activity.shape, dtype=float)
    sum_time = activity.sum(axis=1)
    sum_elem = activity.sum(axis=0)
    # equation (2)
    fraction = sum_time / activity.sum()
    # equation (3) & (4)
    for i in xrange(activity.shape[0]):
        for t in xrange(activity.shape[1]):
            external[i, t] = fraction[i] * sum_elem[t]
            internal[i, t] = activity[i, t] - external[i, t]
    return (internal.std(axis=1, ddof=1), external.std(axis=1, ddof=1))

